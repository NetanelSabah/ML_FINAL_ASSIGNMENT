{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FinalAssignment.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Eu2LaRqYDU5b"},"source":["# Machine Learning Final Assignment\n","\n","The code used for the DREML algorithm and several of the utility functions are based on the following github: https://github.com/leftthomas/DREML\n","\n","The article: Deep Randomized Ensembles for Metric\n","Learning - Hong Xuan, Richard Souvenir,  Robert Pless"]},{"cell_type":"markdown","metadata":{"id":"UHbydT4kTM55"},"source":["## Imports and Mounting"]},{"cell_type":"code","metadata":{"id":"p5tpb2sipFWs"},"source":["import os\n","import torch\n","from scipy.io import loadmat\n","import torch.nn as nn\n","from torchvision.models.resnet import resnet18\n","import copy\n","import random\n","from time import time\n","\n","import sklearn.base\n","import torch.nn.functional as F\n","from torch.nn import CrossEntropyLoss\n","from torch.optim import Adam\n","from torch.optim.lr_scheduler import MultiStepLR\n","from torch.utils.data.dataloader import DataLoader\n","import numpy as np\n","!pip3 install skorch\n","import skorch\n","\n","from sklearn.preprocessing import label_binarize\n","from sklearn.model_selection import KFold, StratifiedKFold\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn import metrics\n","from sklearn import ensemble\n","from imblearn.metrics import specificity_score\n","\n","import random\n","\n","import torch\n","from PIL import Image\n","from torch.utils.data import Dataset\n","from torchvision.datasets import CIFAR100, EMNIST\n","from torchvision import transforms\n","from IPython.display import clear_output\n","\n","from tqdm.notebook import tqdm\n","import math"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CDipW1HihsQQ"},"source":["mount"]},{"cell_type":"code","metadata":{"id":"QLm7PQcPhkh7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627738556788,"user_tz":-180,"elapsed":57062,"user":{"displayName":"In Valid","photoUrl":"","userId":"11584634813269399659"}},"outputId":"eb23f41b-91ba-43b1-818d-db10d3a3466a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0p3kF8VnNUEo"},"source":["RUN_NAME ='Final Run'\n","WORK_DIR = f'/content/drive/MyDrive/ML_FA/{RUN_NAME}'\n","DATASET_DIR = f'/content/drive/MyDrive/ML_FA'\n","MODELS_DIR = f'{WORK_DIR}/model'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kq70diCqpBb3"},"source":["## Pre-processing"]},{"cell_type":"markdown","metadata":{"id":"UG5PrufeTRkN"},"source":["### Data Utils\n","\n","Optional cell used for pre-process the car/cub datasets into .pth files."]},{"cell_type":"code","metadata":{"id":"jpMWmc2aTLSd"},"source":["def read_txt(path):\n","    data = {}\n","    for line in open(path, 'r', encoding='utf-8'):\n","        data_1, data_2 = line.split()\n","        data[data_1] = data_2\n","    return data\n","\n","\n","def process_car_data(data_path):\n","    train_images, test_images = {}, {}\n","    annotations = loadmat('{}/cars_annos.mat'.format(data_path))['annotations'][0]\n","    for img in annotations:\n","        img_name, img_label = str(img[0][0]), str(img[-2][0][0])\n","        if int(img_label) < 99:\n","            if img_label in train_images:\n","                train_images[img_label].append('{}/{}'.format(data_path, img_name))\n","            else:\n","                train_images[img_label] = ['{}/{}'.format(data_path, img_name)]\n","        else:\n","            if img_label in test_images:\n","                test_images[img_label].append('{}/{}'.format(data_path, img_name))\n","            else:\n","                test_images[img_label] = ['{}/{}'.format(data_path, img_name)]\n","    torch.save({'train': train_images, 'test': test_images}, '{}/{}'.format(data_path, data_dicts))\n","\n","\n","def process_cub_data(data_path):\n","    images = read_txt('{}/images.txt'.format(data_path))\n","    labels = read_txt('{}/image_class_labels.txt'.format(data_path))\n","    train_images, test_images = {}, {}\n","    for img_id, img_name in images.items():\n","        if int(labels[img_id]) < 101:\n","            if labels[img_id] in train_images:\n","                train_images[labels[img_id]].append('{}/images/{}'.format(data_path, img_name))\n","            else:\n","                train_images[labels[img_id]] = ['{}/images/{}'.format(data_path, img_name)]\n","        else:\n","            if labels[img_id] in test_images:\n","                test_images[labels[img_id]].append('{}/images/{}'.format(data_path, img_name))\n","            else:\n","                test_images[labels[img_id]] = ['{}/images/{}'.format(data_path, img_name)]\n","    torch.save({'train': train_images, 'test': test_images}, '{}/{}'.format(data_path, data_dicts))\n","\n","\n","# if __name__ == '__main__':\n","#     data_dicts = 'data_dicts.pth'\n","#     process_car_data(f'{DATASET_DIR}/car')\n","#     process_cub_data(f'{DATASET_DIR}/cub')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UJujGYgHTnLo"},"source":["### Utils\n","Some useful functions mainly for pre-processing."]},{"cell_type":"code","metadata":{"id":"VOhqNeZVTsA7"},"source":["\n","def get_transform(data_name, data_type):\n","    normalize = transforms.Normalize(rgb_mean[data_name], rgb_std[data_name])\n","    if data_type == 'train': # apply noise trainsformations to the training images\n","        transform = transforms.Compose([transforms.Resize(int(256 * 1.1)), transforms.RandomCrop(256),\n","                                        transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize])\n","    else:\n","        transform = transforms.Compose(\n","            [transforms.Resize(256), transforms.CenterCrop(256), transforms.ToTensor(), normalize])\n","    return transform\n","\n","\n","# random assign meta class for all classes\n","def create_id(meta_class_size, num_class):\n","    multiple = num_class // meta_class_size\n","    remain = num_class % meta_class_size\n","    if remain != 0:\n","        multiple += 1\n","\n","    idx_all = []\n","    for _ in range(multiple):\n","        idx_base = [j for j in range(meta_class_size)]\n","        random.shuffle(idx_base)\n","        idx_all += idx_base\n","\n","    idx_all = idx_all[:num_class]\n","    random.shuffle(idx_all)\n","    return idx_all\n","\n","\n","def load_data(meta_id, idx_to_class, data_dict):\n","    # balance data for each class\n","    max_size = 300\n","    meta_data_dict = {i: [] for i in range(max(meta_id) + 1)}\n","    for i, c in idx_to_class.items():\n","        meta_class_id = meta_id[i]\n","        image_list = data_dict[c]\n","        if len(image_list) > max_size:\n","            image_list = random.sample(image_list, max_size)\n","        meta_data_dict[meta_class_id] += image_list\n","    return meta_data_dict\n","\n","def get_data_dict(X, y):\n","    \"\"\"\n","      create data_dict based on X and y where the keys are the classes and \n","      the values are a list of image paths of a particular class\n","    \"\"\"\n","    data_dict = {}\n","    for i in range(len(X)):\n","        if y[i] not in data_dict.keys():\n","            data_dict[y[i]] = []\n","        data_dict[y[i]].append(X[i])\n","    return data_dict\n","\n","\n","def get_data_set(dir_path):\n","  \"\"\"\n","    create a data_dict based on a directory of image directories, in the \n","    following structure:\n","    -root\n","    --class_1\n","    ---img_1.png\n","        .\n","        .\n","        .\n","    ---img_n.png\n","    --class_2\n","    ---img_1.png\n","        .\n","        .\n","        .\n","    ---img_n.png\n","      .\n","      .\n","      .\n","    --class_k\n","    ---img_1.png\n","        .\n","        .\n","        .\n","    ---img_n.png\n","  \"\"\"\n","  data_set = {}\n","  labels = os.listdir(dir_path)\n","  for label in tqdm(labels):\n","    label_path = os.path.join(dir_path,\n","                              label)\n","    images = os.listdir(label_path)\n","    images_paths = list(map(lambda image: os.path.join(dir_path, label, image), images))\n","    data_set[label] = images_paths\n","  return data_set\n","\n","def get_hyper_params(params, space):\n","  s = ''\n","  return \", \".join(list(map(lambda key: (f'{key}: {params[key]}')), space)) # for every key of space, get the param of that key.\n","\n","\n","class ImageReader(Dataset):\n","    \"\"\"\n","      This class organizes a data_dict to be fed into a DataLoader \n","      (replacing image paths with the actual values), where the labels\n","      will be replaced with increasing, numeric values.\n","    \"\"\"\n","    def __init__(self, data_dict, transform):\n","        classes = [c for c in sorted(data_dict)]\n","        classes.sort()\n","        class_to_idx = {classes[i]: i for i in range(len(classes))}\n","        self.reverse_class_to_idx = {i: classes[i] for i in range(len(classes))}\n","        self.images, self.labels = [], []\n","        for label in sorted(data_dict):\n","            for img in data_dict[label]:\n","                self.images.append(img)\n","                self.labels.append(class_to_idx[label])\n","        self.transform = transform\n","\n","    def __getitem__(self, index):\n","        path, target = self.images[index], self.labels[index]\n","        img = Image.open(path).convert('RGB')\n","        img = self.transform(img)\n","        return img, target\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def get_label_dictionary(self):\n","      return self.reverse_class_to_idx\n","\n","\n","class LabelConsistentImageReader(Dataset):\n","    \"\"\"\n","      This class organizes a data_dict to be fed into a DataLoader \n","      (replacing image paths with the actual values), while keeping the \n","      labels consistent.\n","    \"\"\"\n","    def __init__(self, data_dict, transform):\n","        self.images, self.labels = [], []\n","        for label in sorted(data_dict):\n","            for img in data_dict[label]:\n","                self.images.append(img)\n","                self.labels.append(label)\n","\n","        self.transform = transform\n","\n","    def __getitem__(self, index):\n","        path, target = self.images[index], self.labels[index]\n","        img = Image.open(path).convert('RGB')\n","        img = self.transform(img)\n","        return img, target\n","\n","    def __len__(self):\n","        return len(self.images)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M211KkHYUFJU"},"source":["### Normalizing\n","Optional cell for finding the parameters (mean, std) for data normalizing:"]},{"cell_type":"code","metadata":{"id":"suEo5f2WqAG8"},"source":["rgb_mean = {'car': [0.4853, 0.4965, 0.4295],\n","            'cub': [0.4707, 0.4601, 0.4549],\n","            'flowers': [0.4326, 0.3730, 0.2803],\n","            'caltech': [0.55, 0.5299, 0.5009]}\n","\n","rgb_std = {'car': [0.2237, 0.2193, 0.2568],\n","           'cub': [0.2767, 0.2760, 0.2850],\n","           'flowers': [0.2972, 0.2442, 0.2651],\n","           'caltech': [0.3185, 0.3152, 0.2383]}\n","\n","flower_t = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n","\n","UNNORMALIZED_DATA = dict()\n","# UNNORMALIZED_DATA['car'] = ImageReader(torch.load(f'{DATASET_DIR}/car/data_dicts.pth'), transform=transforms.ToTensor)\n","# UNNORMALIZED_DATA['flowers'] = ImageReader(get_data_set(f'{DATASET_DIR}/flowers'), transform=flower_t)\n","\n","\n","def calculate_mean_and_std(data_set):\n","  image_loader = DataLoader(data_set, \n","                            batch_size  = 64, \n","                            shuffle     = False, \n","                            num_workers = 1,\n","                            pin_memory  = True)\n","  \n","  ####### COMPUTE MEAN / STD\n","\n","  sum, sum_sq, batches = 0, 0, 0\n","  # loop through images\n","  for inputs, label in tqdm(image_loader):\n","\n","    sum += torch.mean(inputs, dim=[0, 2, 3])\n","    sum_sq += torch.mean(inputs**2, dim=[0, 2, 3])\n","    batches += 1\n","    print(label)\n","\n","  ####### FINAL CALCULATIONS\n","\n","  total_mean = sum / batches\n","  total_std = (sum_sq/batches - total_mean**2) ** 0.5\n","\n","  return total_mean, total_std"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7mOp4nPApNEw"},"source":["## Models"]},{"cell_type":"markdown","metadata":{"id":"TlLNdNM6orv0"},"source":["### Base Model Class\n","The base learning model used in the article - ResNet18"]},{"cell_type":"code","metadata":{"id":"osWI04FAokPq"},"source":["class Model(nn.Module):\n","    def __init__(self, num_class):\n","        super(Model, self).__init__()\n","\n","        # backbone\n","        basic_model, layers = resnet18(pretrained=True), []      # get the basic model\n","        for name, module in basic_model.named_children():\n","            if name == 'fc':                                     # drop the last layer\n","                continue\n","            layers.append(module)\n","        self.features = nn.Sequential(*layers)\n","\n","        # classifier\n","        self.fc = nn.Linear(512, num_class)\n","\n","    def forward(self, x):\n","        x = self.features(x)                                     # predict using the bottom layers\n","        feature = x.view(x.size(0), -1)\n","        out = self.fc(feature)                                   # pass the previous results to the top layer and predict\n","        return out\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5q7E1-_WTwd3"},"source":["### DREML\n","**Stage 1** - A DREML implementation based on the sklearn model interface\n","\n","Note: meta_class_size_rate indirectly refers to D, because D is dependent on the number of classes within the training data.\n","\n","#### Description:\n","The algorithm we have chosen (DREML - Deep Randomized Ensembles for Metric Learning) is an ensemble-based deep learning model for the creation of image-embedding functions in a randomized manner.\n","\n","The main concept that the algorithm is based on, is grouping the class labels of a given dataset into l partitions of d sets each (called meta-classes), where every such set in a given partition contains several different labels chosen randomly. These l partitions constitute an ensemble of l different deep models, where for every partition the image is mapped to the meta-class corresponding to its label. That is essentially what the child model is trained on, which is the mapping between the image and the new meta-class labels that constitute an embedding.\n","\n","At the end of the training process, we have l different embedding functions in the form of the trained child models. The ensemble embedding essentially concatenates all of its children's embeddings into a single, d * l dimensional vector embedding. That can be used via metric learning to indentify groups of images, for either classification, regression or the subject of the article, which is image retrieval.\n","\n","#### Advantages:\n","* The randomized aspect of the ensemble allows for diverse partitions of the classes into meta-classes. This can uncover particularly good meta-class partitions that will significantly boost its metrics, and given this random aspect, repeating the training process of the algorithm might improve results from previous iterations.\n","* Using meta-classes, which in some way represent the relations between random classes of the original data-set, allows for a better understanding of the data-set and might allow us to better identify new, unseen classes that were not included in the training data. For example, if we had trained the algorithm to identify trucks and cars, and a meta-class had included these two labels, then the algorithm might be better at detecting pick-up trucks even if they were not previously included in the data set (since pick-up trucks can be considered an amalgamation of the two).\n","* This type of ensemble considers all information of its child models, and does not disregard the resultant embedding of any child. Thus, improving the performance of the model.\n","\n","#### Disatvantages:\n","* The most glaring issue of the algorithm is in its runtime. Since we are essentially training a whole ensemble of deep learning models within the image-vision field, sometimes up to tens of such models, this comes at a sizable cost of training the model. This forces the end user to adopt more high-computation methods when training the model.\n","* As previously mentioned, the ensemble uses all of its children's embeddings to embed a single image without glossing over any one model. This, in turn, ups the time used to embed the image, which affects the inference time of the model."]},{"cell_type":"code","metadata":{"id":"WNO4fefOT5hi"},"source":["class DREML(sklearn.base.BaseEstimator):\n","    \"\"\"\n","      the DREML algorithm, as adhering to the sklearn model interface.\n","      SVM classifiction stage added.\n","    \"\"\"\n","    def __init__(self, name, ensemble_size=48, meta_class_size_rate=0.25, verbose=False):\n","        self.name = name\n","        if not (0 <= meta_class_size_rate <= 1):\n","          raise AttributeError(\"the meta_class_size_rate either exceeds 1 or is less than 0\")\n","        self.ensemble_size = ensemble_size\n","        self.meta_class_size_rate = meta_class_size_rate\n","        self.meta_class_size = 1\n","        self.models = None\n","        self.verbose = verbose\n","        self.svm_model = None\n","        self.C = None\n","        self.kernel = None\n","        self.param_grid = {'C': [0.1, 1, 10, 100],\n","                           'kernel': ['rbf', 'sigmoid']}\n","\n","    def initialize_models(self, data_dict):\n","      return [{\"model\": Model(self.meta_class_size).to(DEVICE),\n","               # set the meta class partition for the current child model\n","               \"meta_id\": create_id(self.meta_class_size, len(data_dict))}\n","\n","               for i in range(self.ensemble_size)]\n","\n","    def fit(self, X, y, initialize_func=None):\n","        # set the initialize_func to the original value\n","        if initialize_func is None:\n","          initialize_func = self.initialize_models\n","        data_dict = get_data_dict(X, y)\n","        self.num_classes = len(data_dict)\n","        # get D as stated in the article from the rate \n","        self.meta_class_size = math.ceil(self.meta_class_size_rate * self.num_classes)\n","        self.models = initialize_func(data_dict)\n","        i = 0\n","        for i in tqdm(range(1, self.ensemble_size + 1), postfix={'Ensemble': ''}):\n","            all_class = sorted(data_dict)\n","            idx_to_class = {i: all_class[i] for i in range(len(all_class))}\n","            # load the data_dict using the meta_class partition for the current child model\n","            meta_data_dict = load_data(self.models[i-1][\"meta_id\"], idx_to_class, data_dict)\n","\n","            # train the child model using the meta_class partition:\n","            self.optimizer = Adam(self.models[i-1][\"model\"].parameters(), lr=1e-4)\n","            lr_scheduler = MultiStepLR(self.optimizer, milestones=[int(NUM_EPOCHS * 0.5), int(NUM_EPOCHS * 0.7)], gamma=0.1)\n","            self.criterion = CrossEntropyLoss()\n","\n","            best_acc, best_model = 0, None\n","            self.curr_train_acc, self.curr_train_loss, self.best_train_acc= 0.0, 0.0, 0.0\n","            epoch = 0\n","            for epoch in tqdm(range(1, NUM_EPOCHS + 1), postfix={'Epoch': ''}, leave=False):\n","                train_loss, train_acc = self.train(self.models[i-1][\"model\"], meta_data_dict)\n","                self.curr_train_acc = train_acc\n","                self.curr_train_loss = train_loss\n","                # deep copy the model if its performance on the current child meta_class partition is better\n","                if train_acc > best_acc:\n","                    best_acc = train_acc\n","                    self.best_train_acc = best_acc\n","                    best_model = copy.deepcopy(self.models[i-1][\"model\"])\n","                    os.makedirs(f\"{MODELS_DIR}/{self.name}/epochs/\", exist_ok=True)\n","                    torch.save(self.models[i-1][\"model\"].state_dict(), '{}/{}/epochs/{}_model_{:03}.pth'.format(MODELS_DIR, self.name, DATA_NAME, i))\n","                lr_scheduler.step()\n","            self.models[i-1][\"model\"] = best_model\n","        # predict with the child models and get the concatenated coordinates as features\n","        features, labels = self.fit_predict_DREML(data_dict)\n","\n","        #training the svm model based on the returned features\n","        embedded_X_and_y = [(features[i], labels[i]) for i in range(len(features))]\n","        random.shuffle(embedded_X_and_y)\n","        embedded_X, embedded_y =  zip(*embedded_X_and_y)\n","        model = sklearn.svm.SVC(C=self.C, kernel=self.kernel, verbose=self.verbose, probability=True)\n","        cv = KFold(n_splits=3, shuffle=True, random_state=1) # 3 splits\n","        self.svm_model = RandomizedSearchCV(model, self.param_grid, scoring='accuracy', n_jobs=1, cv=cv, refit=True) # define search\n","        results = self.svm_model.fit(embedded_X, embedded_y)\n","        params = results.best_estimator_.get_params()\n","        self.svm_model = results.best_estimator_\n","        self.classes_ = results.best_estimator_.classes_\n","        self.C = params['C']\n","        self.kernel = params['kernel']\n","    \n","\n","    def predict(self, X):\n","        features = self.predict_DREML(X)  # get the concatenated coordinates\n","        y = self.svm_model.predict(features)  # predict by feeding the coordinates to the SVM model\n","        return y\n","\n","    def predict_proba(self, X):\n","        features = self.predict_DREML(X)  # get the concatenated coordinates\n","        y = self.svm_model.predict_proba(features)  # predict_proba by feeding the coordinates to the SVM model\n","        return y\n","\n","    def train(self, net, data_dict):\n","        # a single epoch\n","        net.train()  # set the mode to training\n","        data_set = ImageReader(data_dict, get_transform(DATA_NAME, 'train'))\n","        data_loader = DataLoader(data_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n","\n","        total_size = len(data_set.images)\n","        num_of_batches = total_size // BATCH_SIZE\n","        l_data, t_data, n_data = 0.0, 0, 0 \n","        batch_index = 0\n","        progress_bar = tqdm(data_loader, leave=False, \n","                            postfix={'curr model': self.name,\n","                                    'curr_acc': self.curr_train_acc,\n","                                    'best_acc': self.best_train_acc,\n","                                    'loss': self.curr_train_loss})\n","        for inputs, labels in progress_bar:\n","            # pass the batch\n","            self.optimizer.zero_grad()\n","            out = net(inputs.to(DEVICE))\n","            loss = self.criterion(out, labels.to(DEVICE))\n","            loss.backward()  # calc gradients\n","            self.optimizer.step()  # update weights\n","            # calc values for loss and accuracy\n","            _, pred = torch.max(out, 1)\n","            l_data += loss.item()\n","            t_data += torch.sum(pred.cpu() == labels).item()\n","            n_data += len(labels)\n","\n","        return l_data / n_data, t_data / n_data  # loss, accuracy\n","\n","    def fit_predict_DREML(self, data_dict):\n","        \"\"\"\n","        return two lists with the same length (amount of pictures), where for a \n","        given i the respective entry in the first list will contain the \n","        concatenated coordinates of an image and the entry in the second list \n","        will contain the image's label.\n","        \"\"\"\n","        data_set = LabelConsistentImageReader(data_dict, get_transform(DATA_NAME, 'test'))\n","        model_features = []\n","        all_labels = None\n","        for i in tqdm(range(len(self.models)), leave=True):\n","            self.models[i][\"model\"].eval()\n","            data_loader = DataLoader(data_set, BATCH_SIZE, shuffle=False, num_workers=2)\n","\n","            model_features.append(torch.Tensor([]))\n","            curr_model_features = None\n","            # get the coordinates of every image into model_features[i]\n","            with torch.no_grad():\n","                for inputs, labels in tqdm(data_loader, leave=False):\n","                    # predict the batch\n","                    out = self.models[i][\"model\"](inputs.to(DEVICE))\n","                    out = F.normalize(out)\n","                    out_as_numpy = out.cpu().detach().numpy()\n","                    labels_as_numpy = np.asarray(labels)\n","                    if curr_model_features is None:\n","                        curr_model_features = np.empty(shape=(0, out_as_numpy.shape[1]))\n","                    curr_model_features = np.append(curr_model_features, out_as_numpy, axis=0)\n","                    if all_labels is None:\n","                        all_labels = np.empty(shape=(0), dtype=object)\n","                    all_labels = np.append(all_labels, labels_as_numpy, axis=0)\n","            model_features[i] = torch.from_numpy(curr_model_features)\n","        # concatenate all coordinates in all models of an image\n","        features = torch.cat(model_features, 1) # every row entry represents the coordinates of a single image\n","        return features.detach().numpy(), all_labels\n","\n","    def predict_DREML(self, X):\n","        \"\"\"\n","        return a list where every entry will represent the concatenated coordinates of a single image, to be used as features for the SVM model\n","        \"\"\"\n","        data_dict = get_data_dict(X, [0]*len(X))\n","        data_set = ImageReader(data_dict, get_transform(DATA_NAME, 'test'))\n","        model_features = []\n","        for i in tqdm(range(len(self.models)), leave=True):\n","            self.models[i][\"model\"].eval()\n","            data_loader = DataLoader(data_set, BATCH_SIZE, shuffle=False, num_workers=2)\n","\n","            model_features.append(torch.Tensor([]))\n","            curr_model_features = None\n","            # get the coordinates of every image into model_features[i]\n","            with torch.no_grad():\n","                for inputs, _ in data_loader:\n","                    # predict the batch\n","                    out = self.models[i][\"model\"](inputs.to(DEVICE))\n","                    out = F.normalize(out)\n","                    out_as_numpy = out.cpu().detach().numpy()\n","                    if curr_model_features is None:\n","                        curr_model_features = np.empty(shape=(0, out_as_numpy.shape[1]))\n","                    curr_model_features = np.append(curr_model_features, out.cpu().detach().numpy(), axis=0)\n","            model_features[i] = torch.from_numpy(curr_model_features)\n","        # concatenate all coordinates in all models of an image\n","        features = torch.cat(model_features, 1) # every row entry represents the coordinates of a single image\n","        return features.detach().numpy()\n","\n","    def get_labels(self, data_dict):\n","        data_set = ImageReader(data_dict, get_transform(DATA_NAME, 'test'))\n","        return data_set.labels\n","\n","    def get_images(self, data_dict):\n","        data_set = ImageReader(data_dict, get_transform(DATA_NAME, 'test'))\n","        return data_set.images\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GVt4wegskQJ7"},"source":["### Improved DREML\n","**Stage 2** - In order to improve DREML, we suggest to change D for each child model.\n","\n","Note: This class inherits from the normal DREML since it contains similar code apart from the initialization of the meta_class partitions.\n","\n","#### Description\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"iVg8F_CvkUO8"},"source":["# DREML with changing D of children\n","\n","class Improved_DREML(DREML):\n","  def __init__(self, name, initial_rate=0.05, rate_step=0.01, ensemble_size=24, verbose=False):\n","    super().__init__(name, ensemble_size, initial_rate, verbose)\n","    self.initial_rate = initial_rate\n","    self.rate_step = rate_step\n","\n","    if self.initial_rate + (self.ensemble_size - 1) * self.rate_step > 1 or \\\n","        self.initial_rate < 0 or self.rate_step <= 0:\n","      raise AttributeError(\"The maximum meta_class_size_rate given to the children exceeds 1, or the values of the initial_rate and the rate_step are illegal\") \n","\n","  def varied_initialize_models(self, data_dict):\n","      def meta_class_size_f(k):\n","        return math.ceil((self.initial_rate + k * self.rate_step) * self.num_classes)\n","      return [{\"model\": Model(meta_class_size_f(i)).to(DEVICE),\n","                        \"meta_id\": create_id(meta_class_size_f(i), len(data_dict))}\n","\n","                       for i in range(self.ensemble_size)]\n","\n","  def fit(self, X, y, initialize_func=None):\n","    super().fit(X, y, self.varied_initialize_models)\n","\n","  def predict(self, X):\n","    return super().predict(X)\n","\n","  def predict_proba(self, X):\n","    return super().predict_proba(X)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q48ufM88V4HN"},"source":["### Baseline\n","**Stage 3** - The resnet model, wrapped around skorch to be fed into the sklearn RandomizedSearchCV and used with preprocessing of the ImageReader to convert the X values from the image path into the actual tensor representing that image"]},{"cell_type":"code","metadata":{"id":"qTnaaW1mV7Io"},"source":["class CustomResNet(sklearn.base.BaseEstimator):\n","  def __init__(self, num_class, lr=0.01, optimizer=torch.optim.SGD, batch_size=32):\n","    self.num_class = num_class\n","    self.optimizer = optimizer\n","    self.batch_size = batch_size\n","    self.lr = lr\n","    model = Model(self.num_class).to(DEVICE)\n","    self.skorch_model = skorch.NeuralNet(model,\n","                                         max_epochs=NUM_EPOCHS,\n","                                         optimizer=self.optimizer,\n","                                         batch_size=self.batch_size,\n","                                         lr=self.lr,\n","                                         # Shuffle training data on each epoch\n","                                         iterator_train__shuffle=True,\n","                                         device=DEVICE,\n","                                         criterion=torch.nn.CrossEntropyLoss,\n","                                         verbose=10)\n","    \n","  def fit(self, X, y):\n","    data_dict = get_data_dict(X, y)\n","    data_set = ImageReader(data_dict, get_transform(DATA_NAME, 'train'))\n","    self.class_dictionary = data_set.get_label_dictionary()\n","    self.skorch_model.fit(data_set)\n","    print(f'class_dictionary: {self.class_dictionary}\\nclass_dictionary size: {len(self.class_dictionary)}')\n","    self.classes_ = [self.class_dictionary[i] for i in range(len(np.unique(y)))]\n","    self.params = {'lr': self.lr,\n","                   'optimizer': self.optimizer,\n","                   'batch_size': self.batch_size}\n","\n","  def predict(self, X):\n","    y_preds_probabilities = self.predict_proba(X) # using predict proba to find the values\n","    y_preds_indices = np.argmax(y_preds_probabilities, axis=1)\n","    return list(map(lambda i: self.class_dictionary[i], y_preds_indices))\n","    \n","\n","  def predict_proba(self, X):\n","    data_dict = get_data_dict(X, [0]*len(X))\n","    data_set = ImageReader(data_dict, get_transform(DATA_NAME, 'test'))\n","    val =  self.skorch_model.predict_proba(data_set)\n","    return np.transpose(np.transpose(val) / np.sum(val, axis=1)) # the values in the top layer divided by the sum of the layer\n","\n","\n","def get_ResNet_model(num_class):\n","  return CustomResNet(num_class)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EEYXxACNru2L"},"source":["## Main"]},{"cell_type":"markdown","metadata":{"id":"k-PjU4blQiOZ"},"source":["The data partitions that used to generate the results (Each element is a class label):"]},{"cell_type":"code","metadata":{"id":"v7l9qdpXQf6G"},"source":["partitions = {'car': [\n","                      ['67', '45', '24', '41', '82', '47', '94', '97', '15', '4', '64', '19', '43', '46', '28', '11', '74', '14', '86', '71', '52', '60', '77', '35'],\n","                      ['68', '18', '63', '42', '72', '52', '82', '64', '91', '20', '77', '41', '10', '55', '8', '70', '45', '31', '36', '76', '84', '44', '4', '62'],\n","                      ['42', '96', '65', '6', '19', '60', '47', '97', '85', '76', '26', '51', '40', '52', '88', '91', '3', '77', '25', '78', '34', '87', '31', '32'],\n","                      ['27', '72', '14', '92', '79', '55', '15', '10', '83', '84', '71', '16', '41', '48', '73', '8', '96', '52', '68', '30', '26', '54', '22', '31'],\n","                      ['4', '26', '93', '87', '44', '17', '56', '28', '71', '95', '77', '88', '85', '11', '98', '29', '8', '37', '16', '53', '60', '27', '84', '22']\n","                     ],\n","              'cub': [\n","                      ['11', '24', '65', '56', '57', '22', '48', '58', '72', '10', '43', '97', '90', '49', '75', '68', '55', '16', '37', '54', '40', '39', '98', '34'],\n","                      ['97', '3', '43', '94', '82', '27', '93', '26', '7', '28', '4', '96', '63', '2', '51', '89', '52', '37', '62', '47', '11', '68', '5', '85'],\n","                      ['52', '70', '21', '95', '48', '58', '57', '65', '69', '11', '1', '98', '73', '66', '13', '93', '80', '30', '14', '77', '97', '64', '24', '22'],\n","                      ['68', '41', '99', '94', '38', '76', '2', '80', '93', '69', '79', '24', '14', '58', '100', '31', '19', '18', '92', '12', '70', '36', '20', '71'],\n","                      ['58', '25', '32', '84', '72', '89', '87', '55', '95', '4', '100', '28', '51', '1', '2', '14', '67', '36', '43', '73', '88', '44', '92', '9']\n","                     ],\n","              'flowers': [\n","                          ['80', '44', '83', '2', '71', '56', '46', '15', '11', '72', '20', '70', '75', '63', '14', '90', '51', '87', '77', '55', '17', '5', '37', '66', '30', '36', '49', '52', '85'],\n","                          ['92', '52', '14', '48', '18', '56', '61', '12', '63', '20', '86', '88', '60', '91', '87', '42', '41', '23', '36', '77', '30', '22', '93', '32', '37', '80', '44', '83', '5'],\n","                          ['87', '37', '92', '75', '65', '14', '20', '63', '47', '60', '15', '12', '73', '81', '68', '71', '38', '44', '64', '93', '84', '42', '74', '49', '91', '18', '46', '59', '19'],\n","                          ['76', '66', '53', '38', '74', '18', '12', '75', '19', '60', '37', '87', '4', '47', '54', '59', '77', '70', '51', '69', '29', '91', '93', '48', '11', '85', '90', '78', '32'],\n","                          ['74', '14', '20', '42', '8', '52', '92', '38', '43', '5', '19', '22', '28', '91', '73', '41', '64', '4', '18', '89', '93', '56', '47', '44', '46', '84', '59', '23', '51']\n","                         ],\n","              'caltech': [\n","                          ['068.fern', '172.revolver-101', '193.soccer-ball', '191.sneaker', '189.snail', '123.ketch-101', '082.galaxy', '149.necktie', '165.pram', '212.teapot', '089.goose', '091.grand-piano-101', '037.chess-board', '032.cartman', '097.harmonica', '218.tennis-racket', '169.radio-telescope', '115.ice-cream-cone', '081.frying-pan', '111.house-fly', '058.doorknob', '155.paperclip', '018.bowling-pin'],\n","                          ['138.mattress', '123.ketch-101', '021.breadmaker', '180.screwdriver', '154.palm-tree', '113.hummingbird', '141.microscope', '091.grand-piano-101', '167.pyramid', '107.hot-air-balloon', '076.football-helmet', '175.roulette-wheel', '178.school-bus', '111.house-fly', '189.snail', '077.french-horn', '093.grasshopper', '165.pram', '135.mailbox', '025.cactus', '046.computer-monitor', '204.sunflower-101', '068.fern'],\n","                          ['101.head-phones', '110.hourglass', '207.swan', '132.light-house', '002.american-flag', '173.rifle', '167.pyramid', '200.stained-glass', '196.spaghetti', '177.saturn', '104.homer-simpson', '040.cockroach', '140.menorah-101', '041.coffee-mug', '138.mattress', '004.baseball-bat', '199.spoon', '082.galaxy', '175.roulette-wheel', '037.chess-board', '100.hawksbill-101', '081.frying-pan', '018.bowling-pin'],\n","                          ['069.fighter-jet', '200.stained-glass', '192.snowmobile', '216.tennis-ball', '037.chess-board', '224.touring-bike', '182.self-propelled-lawn-mower', '102.helicopter-101', '122.kayak', '132.light-house', '170.rainbow', '142.microwave', '085.goat', '095.hamburger', '065.elk', '163.playing-card', '035.cereal-box', '160.pez-dispenser', '083.gas-pump', '077.french-horn', '012.binoculars', '038.chimp', '169.radio-telescope'],\n","                          ['033.cd', '006.basketball-hoop', '027.calculator', '078.fried-egg', '189.snail', '029.cannon', '210.syringe', '059.drinking-straw', '034.centipede', '168.raccoon', '154.palm-tree', '145.motorbikes-101', '019.boxing-glove', '209.sword', '091.grand-piano-101', '030.canoe', '184.sheet-music', '102.helicopter-101', '040.cockroach', '049.cormorant', '211.tambourine', '127.laptop-101', '119.jesus-christ']\n","                         ]\n","              }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"byv1pQT8UGk7"},"source":["DATA_NAMES, BATCH_SIZE, NUM_EPOCHS = ['car', 'flowers', 'cub', 'caltech'], 32, 12\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Normalization Results for each dataset:\n","rgb_mean = {'car': [0.4853, 0.4965, 0.4295],\n","            'cub': [0.4707, 0.4601, 0.4549],\n","            'flowers': [0.4326, 0.3730, 0.2803],\n","            'caltech': [0.55, 0.5299, 0.5009]}\n","\n","rgb_std = {'car': [0.2237, 0.2193, 0.2568],\n","           'cub': [0.2767, 0.2760, 0.2850],\n","           'flowers': [0.2972, 0.2442, 0.2651],\n","           'caltech': [0.3185, 0.3152, 0.2383]}\n","\n","\n","\n","def data_to_Xy_and_classes(data):\n","  # generate a random, sampling partition of the current dataset\n","  Xy = []\n","  X = []\n","  y = []\n","  classes_list = generate_classes_list(data)\n","  for label in tqdm(data['data'], 'Pre-processing: Randomly partitioning the dataset...'):\n","    if label in classes_list:\n","      # for every class included in the partition, take the first values based on the inclass_use_rate\n","      length = int(len(data['data'][label]) * data['inclass_use_rate']) \n","      X.extend(data['data'][label][: length])\n","      y.extend([label]*len(data['data'][label][: length]))\n","  return np.asarray(X), np.asarray(y), classes_list\n","\n","def generate_classes_list(data):\n","  # generate a list representing a partition\n","  num_of_trunc_classes = math.ceil(data['num_classes'] * data['class_use_rate'])\n","  initial_list = list(data['data'].keys())\n","  random.shuffle(initial_list)\n","  lis = initial_list[: num_of_trunc_classes]\n","  os.makedirs(f\"{MODELS_DIR}/results/{DATA_NAME}\", exist_ok=True)\n","  f = open(f'{MODELS_DIR}/results/{DATA_NAME}/{partition}_description.txt', 'w')\n","  f.write(str(lis))\n","  f.close()\n","  return lis\n","\n","models = {'DREML': {'model_factory': DREML,\n","                    'space': {'ensemble_size': [8, 12, 24],\n","                              'meta_class_size_rate': [0.1, 0.2, 0.3]}},\n","          'resnet': {'model_factory': lambda name, verbose: get_ResNet_model(num_class),\n","                     'space': {'lr': [0.1, 0.01, 0.005, 0.001, 0.0005],\n","                               'optimizer': [torch.optim.SGD, torch.optim.Adam, torch.optim.RMSprop],\n","                               'batch_size': [16, 32, 64, 128]}},\n","          'improved_DREML': {'model_factory': Improved_DREML,\n","                             'space': {'ensemble_size': [8, 12, 24],\n","                                       'initial_rate': [0.02],\n","                                       'rate_step': [0.005, 0.01, 0.02]}}}\n","\n","DATA = {'caltech': {'data': get_data_set(f'{DATASET_DIR}/caltech/256_ObjectCategories'),\n","                    'class_use_rate': 0.5,\n","                    'num_classes': 225,\n","                    'inclass_use_rate': 0.25},\n","        'car': {'data': torch.load(f'{DATASET_DIR}/car/data_dicts.pth')['train'],\n","                'class_use_rate': 0.12,\n","                'num_classes': 196,\n","                'inclass_use_rate': (1/3)},\n","        'cub': {'data': torch.load(f'{DATASET_DIR}/cub/data_dicts.pth')['train'],\n","                'class_use_rate': 0.12,\n","                'num_classes': 200,\n","                'inclass_use_rate': 0.25},\n","        'flowers': {'data': get_data_set(f'{DATASET_DIR}/flowers'),\n","                    'class_use_rate': 0.4,\n","                    'num_classes': 71,\n","                    'inclass_use_rate': 0.25}}\n","s = ''\n","for DATA_NAME in DATA_NAMES:\n","  for partition in range(5):\n","\n","    data = DATA[DATA_NAME]  # data contains a dictionary where the data itself, class_use_rate, num_classes and inclass_use_rate can be obtained\n","    X, y, classes_list = data_to_Xy_and_classes(data)  # generate a random, sampling partition of the current dataset\n","    classes_list.sort()\n","    num_class = len(classes_list)\n","\n","    for model_name in models:\n","      print(f'train {model_name} on {DATA_NAME}_{partition}')\n","      model_type = models[model_name]\n","      cv_outer = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n","      outer_results = list()\n","      i = 0\n","\n","      for train_ix, test_ix in cv_outer.split(X, y):\n","          i += 1\n","\n","          X_train, X_test = X[train_ix], X[test_ix]\n","          y_train, y_test = y[train_ix], y[test_ix]\n","          cv_inner = KFold(n_splits=2, shuffle=True, random_state=1)\n","          model = model_type['model_factory'](name=f'{DATA_NAME}_{partition}',verbose=True)\n","          space = model_type['space']\n","          search = RandomizedSearchCV(model, space, scoring='accuracy', n_jobs=1, cv=cv_inner, refit=True, verbose=10)\n","\n","          train_time = time()\n","          result = search.fit(X_train, y_train) # cross validate (find hyper params) and fit\n","          train_time = time() - train_time\n","\n","          best_model = result.best_estimator_\n","          \n","          print(f'predict of {model_name} on {DATA_NAME}_{partition}')\n","          test_time = time()\n","          yhat = best_model.predict(X_test)\n","          test_time = (time() - test_time) * 1000 / len(y_test)\n","\n","          print(f'predict_proba of {model_name} on {DATA_NAME}_{partition}')\n","          y_proba = best_model.predict_proba(X_test)\n","\n","          # calculate results:\n","\n","          acc = metrics.balanced_accuracy_score(y_test, yhat) # uses the balanced accuracy function\n","          print(f'acc: {acc}')\n","          \n","          tpr = metrics.recall_score(y_test, yhat, average='weighted')\n","          fpr = 1 - specificity_score(y_test, yhat, average='weighted')\n","\n","          precision = metrics.precision_score(y_test, yhat, average='weighted')\n","          \n","          y_pred_calc = [model.classes_[np.argmax(y_proba[i], axis=0)] for i  in range(y_proba.shape[0])]\n","          auc = metrics.roc_auc_score(y_test, y_proba, average='weighted', multi_class='ovr', labels=list(np.unique(y_test)))\n","          auprc = metrics.average_precision_score(label_binarize(y_test, classes_list), label_binarize(yhat, classes_list), average=\"weighted\")\n","          expanded_spaces = ['C', 'kernel']\n","          expanded_spaces.extend(list(space.keys()))\n","\n","          s += f'Model Name: {model_name}\\n'\\\n","            + f'Data Name: {DATA_NAME}-{partition}\\n'\\\n","            + f'Cross Validation: {i}\\n'\\\n","            + f'Hyper-Parameters: {get_hyper_params(best_model.params, expanded_spaces)}\\n'\\\n","            + f'Accuracy: {acc}\\n'\\\n","            + f'TPR: {tpr}\\n'\\\n","            + f'FPR: {fpr}\\n'\\\n","            + f'Precision: {precision}\\n'\\\n","            + f'AUC: {auc}\\n'\\\n","            + f'Area under the Precision-Recall Curve: {auprc}\\n'\\\n","            + f'Training Time: {train_time}\\n'\\\n","            + f'Inference Time: {test_time}\\n\\n'\n","          os.makedirs(f\"{MODELS_DIR}/results\", exist_ok=True)\n","          f = open(f'{MODELS_DIR}/results/{model_name}_{DATA_NAME}-{partition}_{i}.txt', 'w')  # save the results\n","          f.write(s)\n","          f.close()\n","\n","          clear_output() # clean the screen\n","          print(s)\n"," "],"execution_count":null,"outputs":[]}]}